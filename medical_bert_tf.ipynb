{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "medical_bert_tf.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1BxS3wP9hkIQ5a-ZuxhtHFKyp0Xb5aRSZ",
      "authorship_tag": "ABX9TyMvbh2CcxeEdVTGLqDukeT+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tyanakai/medical_paper_classification/blob/main/medical_bert_tf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7y8qO3ZzvALT"
      },
      "source": [
        "## config etc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1YRVdjyrNbT"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHvKphT8BMKE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02a8aaa4-a0aa-41a4-ff5a-c65bde2711c5"
      },
      "source": [
        "! pip install -q transformers\n",
        "! pip install -q tensorflow-addons\n",
        "\n",
        "import datetime\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from scipy.optimize import minimize\n",
        "from scipy.optimize import minimize_scalar\n",
        "from sklearn.metrics import fbeta_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.utils import class_weight\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow_addons as tfa\n",
        "import transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.9 MB 4.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 636 kB 43.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 78.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 49.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 5.3 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBKkHi54SQSc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a3c51432-30f4-4d14-ed90-7e45c20ba699"
      },
      "source": [
        "class Config:\n",
        "    model = \"dmis-lab/biobert-base-cased-v1.2\" #@param\n",
        "    from_pt = True #@param {\"type\":\"boolean\"}\n",
        "    encode_type = \"cls_cat\" #@param {\"type\",\"string\"} [\"cls\",\"cls_cat\",\"pooler\",\"logits\", \"last_hidden_state_cnn\", \"last_hidden_state_lstm\"]\n",
        "\n",
        "    max_length = 512 #@param {\"type\":\"integer\"}\n",
        "    lr = 0.00002\n",
        "    weight_decay = 1e-5\n",
        "    opt = \"minimize\"  #@param {\"type\":\"string\"} [\"minimize_scalar\",\"minimize\"]\n",
        "    n_fold = 5 #@param\n",
        "    epochs = 15 #@param {\"type\":\"slider\"}\n",
        "    patience =  4#@param\n",
        "    check_monitor = \"val_fbeta_score\" #@param {\"type\":\"string\"} [\"val_loss\",\"val_fbeta_score\",\"val_auc\"]\n",
        "    check_mode = \"max\" #@param {\"type\":\"string\"} [\"auto\", \"max\"]\n",
        "    \n",
        "    train_batch_size = 64 #@param {\"type\":\"raw\"} [4,8,16,32,64]\n",
        "    valid_batch_size = 64 #@param {\"type\":\"raw\"} [4,8,16,32,64]\n",
        "    test_batch_size = 64 #@param {\"type\":\"raw\"} [4,8,16,32,64]\n",
        "    steps_per_epochs = None #(27145 * (n_fold - 1) / n_fold) // train_batch_size\n",
        "    train_file = \"ps_train.csv\" #@param\n",
        "    test_file = \"ps_test.csv\" #@param\n",
        "    target_col = \"judgement\"\n",
        "    text_col = \"text\"\n",
        "    seeds = [21]\n",
        "\n",
        "    loss_fn = \"bce\" #@param {\"type\":\"string\"} [\"bce\", \"weighted_bce\", \"focal\"]\n",
        "    loss_weight = [1, 50] #@param\n",
        "    class_weight = \"balanced\" #@param {\"type\":\"raw\"} \n",
        "    sample_weight = None #@param    \n",
        "    label_smoothing = 0 #@param\n",
        "    \n",
        "    submit = True #@param {\"type\":\"boolean\"}\n",
        "    debug = False  #@param {\"type\":\"boolean\"}\n",
        "    temp_thre = 0.1 #@param\n",
        "\n",
        "if Config.debug:\n",
        "    Config.epochs = 2\n",
        "    Config.n_fold = 2\n",
        "\n",
        "time_jp = (datetime.datetime.now() + \n",
        "           datetime.timedelta(hours=9)).strftime('%Y%m%d_%H%M')\n",
        "# time_jp = '20210926_1749' #@param\n",
        "time_jp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'20211004_2013'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqEOf5pnSx18"
      },
      "source": [
        "def build_encoder():\n",
        "    \"\"\"\n",
        "    encoderの出力形式(Config.encode_type)に従って\n",
        "    設定を変化させたencoderを返します。\n",
        "    \"\"\"\n",
        "    if Config.encode_type == \"logits\":\n",
        "        encoder = (\n",
        "            transformers\n",
        "            .TFAutoModelForSequenceClassification\n",
        "            .from_pretrained(Config.model, num_labels=1, from_pt=Config.from_pt)\n",
        "        )\n",
        "\n",
        "    elif Config.encode_type == \"cls_cat\":\n",
        "        config = transformers.AutoConfig.from_pretrained(Config.model,\n",
        "                                                         output_hidden_states=True)\n",
        "        encoder = (\n",
        "            transformers\n",
        "            .TFAutoModel\n",
        "            .from_pretrained(Config.model, config=config, from_pt=Config.from_pt)\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        encoder = (\n",
        "            transformers\n",
        "            .TFAutoModel\n",
        "            .from_pretrained(Config.model, from_pt=Config.from_pt)\n",
        "        )\n",
        "\n",
        "    return encoder\n",
        "\n",
        "def neural_networks(x):\n",
        "    \"\"\"\n",
        "    encoderの出力形式(Config.encode_type)に従って\n",
        "    encoder以降の構造を定義します。\n",
        "    \"\"\"\n",
        "\n",
        "    if Config.encode_type == \"cls\":\n",
        "        x = x[0][:, 0, :]  # cls token\n",
        "        output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    elif Config.encode_type == \"cls_cat\":\n",
        "        # encoderの最終四層分のcls tokenを連結します。\n",
        "        x = tf.concat([x[\"hidden_states\"][-i][:,0,:] for i in range(1,5)], axis=-1)\n",
        "        x = tf.keras.layers.Dropout(0.2)(x)\n",
        "        output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    elif Config.encode_type == \"pooler\":\n",
        "        x = x[\"pooler_output\"]\n",
        "        output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    elif Config.encode_type == \"logits\":\n",
        "        x = x.logits\n",
        "        output = tf.keras.layers.Activation(\"sigmoid\")(x)\n",
        "\n",
        "    elif Config.encode_type == \"last_hidden_state_cnn\":\n",
        "        # encoderの最終出力を１次元のCNNで処理します。\n",
        "        x = x.last_hidden_state\n",
        "        x = tf.keras.layers.Conv1D(\n",
        "            256, kernel_size=2, padding=\"same\", activation=\"relu\")(x)\n",
        "        x = tf.keras.layers.Conv1D(\n",
        "            1, kernel_size=2, padding=\"same\")(x)\n",
        "        x = tf.keras.layers.GlobalMaxpooling1D()(x)\n",
        "        output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "        \n",
        "    elif Config.encode_type == \"last_hidden_state_lstm\":\n",
        "        # encoderの最終出力を双方向LSTMで処理します。\n",
        "        x = x.last_hidden_state\n",
        "        x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(728))(x)\n",
        "        output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "    \n",
        "    return output\n",
        "\n",
        "\n",
        "def build_model():\n",
        "    \"\"\"\n",
        "    使用するkerasモデルの全体像を定義します。\n",
        "    \"\"\"\n",
        "    # encoder\n",
        "    encoder = build_encoder()\n",
        "\n",
        "    # 入力\n",
        "    input_ids = tf.keras.layers.Input(shape=(Config.max_length, ), \n",
        "                                           dtype=tf.int32, \n",
        "                                           name='input_ids')\n",
        "    attention_mask = tf.keras.layers.Input(shape=(Config.max_length, ),\n",
        "                                           dtype=tf.int32, \n",
        "                                           name='attention_mask')\n",
        "    \n",
        "    # ニューラルネットワーク全体構造\n",
        "    x = encoder(input_ids=input_ids, \n",
        "                attention_mask=attention_mask, \n",
        "                output_hidden_states=True)\n",
        "    output = neural_networks(x)\n",
        "\n",
        "    # kerasモデル化\n",
        "    model = tf.keras.models.Model(inputs=[input_ids, attention_mask],\n",
        "                                  outputs=[output])\n",
        "\n",
        "    # 最適化アルゴリズムと損失関数\n",
        "    optimizer = tfa.optimizers.AdamW(lr=Config.lr, weight_decay=Config.weight_decay)\n",
        "    loss = {\"bce\": tf.keras.losses.BinaryCrossentropy(),\n",
        "            \"weighted_bce\": weighted_binary_crossentropy(Config.loss_weight, Config.label_smoothing),\n",
        "            \"focal\": tfa.losses.SigmoidFocalCrossEntropy(alpha=0.98, gamma=2.0),\n",
        "            \"mse\": tf.keras.losses.MeanSquaredError()}\n",
        "\n",
        "    # 訓練中監視する指標\n",
        "    metrics = [tfa.metrics.FBetaScore(num_classes=1,\n",
        "                                      beta=7.0,\n",
        "                                      threshold=Config.temp_thre),\n",
        "               tf.keras.metrics.AUC(num_thresholds=200, curve='PR',\n",
        "                                    multi_label=False, label_weights=None)]\n",
        "\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss=loss[Config.loss_fn], \n",
        "                  metrics=metrics)\n",
        "    # model.summary()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1O9-YHdofe6H"
      },
      "source": [
        "# pathの設定\n",
        "DRIVE = \"/content/drive/MyDrive/signate/medical_paper\"\n",
        "INPUT = os.path.join(DRIVE, \"input\")\n",
        "OUTPUT = os.path.join(DRIVE, \"output\")\n",
        "LOG = os.path.join(OUTPUT, f\"{Config.model.replace('/','-')}_tf\")\n",
        "MODEL = os.path.join(DRIVE, \"model\", f\"{Config.model.replace('/','-')}_tf\")\n",
        "SUBMIT = os.path.join(DRIVE, \"submit\")\n",
        "PROB = os.path.join(DRIVE, \"prob\")\n",
        "\n",
        "os.makedirs(MODEL, exist_ok=True)\n",
        "os.makedirs(LOG, exist_ok=True)\n",
        "\n",
        "# warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nQXayCa7XaO",
        "outputId": "854078be-9530-4b17-82c7-9a4b389edae3"
      },
      "source": [
        "# Loggerの設定\n",
        "class Logger:\n",
        "    \"\"\"log を残す用のクラス\"\"\"\n",
        "    def __init__(self, path):\n",
        "        self.general_logger = logging.getLogger(__name__)\n",
        "        stream_handler = logging.StreamHandler()\n",
        "        file_general_handler = logging.FileHandler(os.path.join(path, f'Experiment{time_jp}.log'))\n",
        "        if len(self.general_logger.handlers) == 0:\n",
        "            # self.general_logger.addHandler(stream_handler)\n",
        "            self.general_logger.addHandler(file_general_handler)\n",
        "            self.general_logger.setLevel(logging.INFO)\n",
        "\n",
        "    def info(self, message):\n",
        "        # display time\n",
        "        self.general_logger.info('[{}] - {}'.format(self.now_string(), message))\n",
        "\n",
        "    @staticmethod\n",
        "    def now_string():\n",
        "        cur_time = datetime.datetime.now() + datetime.timedelta(hours=9)\n",
        "        return cur_time.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "logger = Logger(LOG)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on TPU  ['10.49.54.90:8470']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quPdMBltfWeR"
      },
      "source": [
        "# TPU設定\n",
        "try:\n",
        "    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "    print('Running on TPU ', TPU.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "    TPU = None\n",
        "    print('INFO: Not connected to a TPU runtime')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpeaSp39-9ai"
      },
      "source": [
        "# data準備\n",
        "\n",
        "def get_data(file_name):\n",
        "    df = pd.read_csv(os.path.join(INPUT, file_name))\n",
        "    if Config.debug:\n",
        "        df = df.sample(256, random_state=Config.seeds[0]).reset_index(drop=True)\n",
        "\n",
        "    # preprocess\n",
        "    df[\"text\"] = df[\"title\"] + \" \" + df[\"abstract\"].fillna(\"\")\n",
        "    return df\n",
        "\n",
        "def skf(train, n_splits, random_state):\n",
        "    \"\"\"\n",
        "    層化K分割したindexのリストを返す\n",
        "    \"\"\"\n",
        "    if n_splits > 1:\n",
        "        skf = StratifiedKFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n",
        "        return list(skf.split(train, train[Config.target_col]))\n",
        "    else:\n",
        "        return train.index\n",
        "\n",
        "\n",
        "def tokenize_texts(texts, tokenizer, max_length):\n",
        "    \"\"\"\n",
        "    keyが\"input_ids\"と\"attention_mask\"の辞書を返す\n",
        "    \"\"\"\n",
        "    tokenized_dict = tokenizer.batch_encode_plus(\n",
        "        texts,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_token_type_ids=False,\n",
        "    )\n",
        "    return dict(tokenized_dict)\n",
        "\n",
        "\n",
        "def get_dataset(X, y=None, dataset=\"test\"):\n",
        "    \"\"\"データをtf.data.Datasetの形式に変更\"\"\"\n",
        "\n",
        "    if dataset==\"train\":\n",
        "        tr_ds = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "        if Config.steps_per_epochs is not None:\n",
        "            tr_ds = tr_ds.repeat()\n",
        "        tr_ds = tr_ds.shuffle(2048)\n",
        "        tr_ds = tr_ds.batch(Config.train_batch_size)\n",
        "        tr_ds = tr_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "        return tr_ds\n",
        "\n",
        "    elif dataset==\"valid\":\n",
        "        val_ds = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "        val_ds = val_ds.batch(Config.valid_batch_size)\n",
        "        val_ds = val_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "        return val_ds\n",
        "    \n",
        "    elif dataset==\"test\":\n",
        "        test_ds = tf.data.Dataset.from_tensor_slices(X)\n",
        "        test_ds = test_ds.batch(Config.test_batch_size)\n",
        "        test_ds = test_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "        return test_ds\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6AmkOBbSyfC"
      },
      "source": [
        "def weighted_binary_crossentropy(weight, label_smoothing):\n",
        "    \"\"\"\n",
        "    label毎に異なる重みを付加したcrossentropy loss関数\n",
        "    \"\"\"\n",
        "    weight = tf.convert_to_tensor(weight, dtype=tf.float32)\n",
        "\n",
        "    def _weighted_binary_crossentropy(target, output):\n",
        "        \"\"\"\n",
        "        label smoothingに対応した一般的なcrossentropy loss関数\n",
        "        keras公式の実装を参考に実装\n",
        "\n",
        "        \"\"\"\n",
        "        if Config.label_smoothing:\n",
        "            target = target * (1.0 - label_smoothing) + 0.5 * label_smoothing\n",
        "        target = tf.convert_to_tensor(target, dtype=tf.float32)\n",
        "        target = tf.reshape(target, [-1])\n",
        "\n",
        "        output = tf.convert_to_tensor(output, dtype=tf.float32)\n",
        "        output = tf.reshape(output, [-1])\n",
        "        epsilon_ = K.epsilon()\n",
        "        output = tf.clip_by_value(output, epsilon_, 1. - epsilon_)\n",
        "\n",
        "        bce = weight[1] * target * tf.math.log(output + K.epsilon())\n",
        "        bce += weight[0] * (1 - target) * tf.math.log(1 - output + K.epsilon())\n",
        "        return -bce\n",
        "\n",
        "    return _weighted_binary_crossentropy\n",
        "\n",
        "\n",
        "def opt_fbeta_threshold(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    fbeta scoreに対する閾値を最適化\n",
        "    \"\"\"\n",
        "    def opt_(x): \n",
        "        return -fbeta_score(y_true, y_pred>=x, beta=7)\n",
        "    if Config.opt == \"minimize\":\n",
        "        result = minimize(opt_, x0=np.array([0.1]), method=\"Powell\")\n",
        "    elif Config.opt == \"minimize_scalar\":\n",
        "        result = minimize_scalar(opt_, bounds=(0.001, 0.85), method='bounded')\n",
        "    opted_threshold = result['x'].item()\n",
        "    return opted_threshold\n",
        "\n",
        "\n",
        "def metrics(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    最適化された閾値とその時のfbeta scoreを取得。\n",
        "    \"\"\"\n",
        "    opted_thre = opt_fbeta_threshold(y_true, y_pred)\n",
        "    print(f\"opted threshold : {opted_thre}\")\n",
        "    score = fbeta_score(y_true, y_pred >= opted_thre, beta=7)\n",
        "    return score, opted_thre\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_model_and_tokenizer():\n",
        "    \"\"\"\n",
        "    modelとtokenizerを取得\n",
        "    \"\"\"\n",
        "    if TPU:\n",
        "        tf.config.experimental_connect_to_cluster(TPU)\n",
        "        tf.tpu.experimental.initialize_tpu_system(TPU)\n",
        "        tpu_strategy = tf.distribute.TPUStrategy(TPU)\n",
        "        with tpu_strategy.scope():\n",
        "            model = build_model()\n",
        "    else:\n",
        "        model = build_model()\n",
        "\n",
        "    tokenizer = transformers.AutoTokenizer.from_pretrained(Config.model)\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "def get_class_weight(target, weight):\n",
        "    \"\"\"\n",
        "    class weightを取得する。\n",
        "    \"\"\"\n",
        "    if weight == \"balanced\":\n",
        "        class_weights = class_weight.compute_class_weight(\n",
        "            class_weight='balanced',\n",
        "            classes=np.unique(target),\n",
        "            y=target)\n",
        "        class_weights = dict(enumerate(class_weights))\n",
        "    elif weight is None:\n",
        "        class_weights = None\n",
        "    else:\n",
        "        # ex) weight = {0:0.2, 1:0.98}\n",
        "        class_weights = weight\n",
        "    \n",
        "    return class_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cBPoNKgEJ_k"
      },
      "source": [
        "# 訓練結果の表示\n",
        "def visualize_confusion_matrix(\n",
        "        y_true,\n",
        "        pred_label,\n",
        "        height=.6,\n",
        "        labels=None):\n",
        "    \"\"\"\n",
        "    混合行列を表示\n",
        "    \"\"\"\n",
        "    conf = confusion_matrix(y_true=y_true,\n",
        "                            y_pred=pred_label,\n",
        "                            normalize='true')\n",
        "\n",
        "    n_labels = len(conf)\n",
        "    size = n_labels * height\n",
        "    fig, ax = plt.subplots(figsize=(size * 4, size * 3))\n",
        "    sns.heatmap(conf, cmap='Blues', ax=ax, annot=True, fmt='.2f')\n",
        "    ax.set_ylabel('Label')\n",
        "    ax.set_xlabel('Predict')\n",
        "\n",
        "    if labels is not None:\n",
        "        ax.set_yticklabels(labels)\n",
        "        ax.set_xticklabels(labels)\n",
        "        ax.tick_params('y', labelrotation=0)\n",
        "        ax.tick_params('x', labelrotation=90)\n",
        "\n",
        "    plt.show()\n",
        "    return fig\n",
        "\n",
        "\n",
        "\n",
        "def get_result(y_true, y_pred, thresholds=None):\n",
        "    thre_df = pd.DataFrame(columns=[\"threshold\", \"score\"])\n",
        "\n",
        "    naive_score = fbeta_score(y_true, y_pred >= Config.temp_thre, beta=7.0)\n",
        "    opted_score, opted_thre = metrics(y_true, y_pred)\n",
        "    type_list = [\"opted\", \"temporary\"]\n",
        "    thre_list = [opted_thre, Config.temp_thre]\n",
        "    score_list = [opted_score, naive_score]\n",
        "\n",
        "    if type(thresholds) is list:\n",
        "        thresholds = np.array(thresholds)\n",
        "        min_thre = thresholds.min()\n",
        "        max_thre = thresholds.max()\n",
        "        mean_thre = thresholds.mean()\n",
        "        med_thre = np.median(thresholds)\n",
        "\n",
        "        min_score = fbeta_score(y_true, y_pred >= min_thre, beta=7.0)\n",
        "        max_score = fbeta_score(y_true, y_pred >= max_thre, beta=7.0)\n",
        "        mean_score = fbeta_score(y_true, y_pred >= mean_thre, beta=7.0)\n",
        "        med_score = fbeta_score(y_true, y_pred >= med_thre, beta=7.0)\n",
        "\n",
        "        type_list += [\"min\", \"max\", \"mean\",\"median\"]\n",
        "        thre_list += [min_thre, max_thre, mean_thre, med_thre]\n",
        "        score_list += [min_score, max_score, mean_score, med_score] \n",
        "\n",
        "    thre_df[\"threshold\"] = thre_list\n",
        "    thre_df[\"score\"] = score_list\n",
        "    thre_df.index = type_list\n",
        "\n",
        "    return thre_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brnebJMGtIds"
      },
      "source": [
        "def train_fn(train_df, valid_df, model, tokenizer, filepath):\n",
        "    \"\"\"\n",
        "    訓練関数\n",
        "    \"\"\"\n",
        "    # data準備\n",
        "    tr_text = tokenize_texts(texts=train_df[Config.text_col].tolist(), tokenizer=tokenizer, max_length=Config.max_length)\n",
        "    val_text = tokenize_texts(texts=valid_df[Config.text_col].tolist(), tokenizer=tokenizer, max_length=Config.max_length)\n",
        "\n",
        "    tr_dataset = get_dataset(X=tr_text, y=train_df[Config.target_col].values, dataset=\"train\")\n",
        "    val_dataset = get_dataset(X=va_text, y=valid_df[Config.target_col].values, dataset=\"valid\")\n",
        "    \n",
        "    # callbacks\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath, \n",
        "        monitor=Config.check_monitor, \n",
        "        verbose=1, \n",
        "        save_best_only=True, \n",
        "        save_weights_only=True,\n",
        "        mode=Config.check_mode\n",
        "        )\n",
        "    \n",
        "    earlystop = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_loss\",\n",
        "        patience=Config.patience\n",
        "        )\n",
        "    \n",
        "    # fit\n",
        "    history = model.fit(\n",
        "        tr_dataset, \n",
        "        epochs=Config.epochs, \n",
        "        verbose=1, \n",
        "        callbacks=[checkpoint, earlystop],\n",
        "        validation_data=val_dataset, \n",
        "        steps_per_epoch=Config.steps_per_epochs,\n",
        "        class_weight=get_class_weight(train_df[Config.target_col], weight=Config.class_weight)\n",
        "        )\n",
        "    \n",
        "    return history\n",
        "\n",
        "\n",
        "def inference_fn(test_df, model, tokenizer, filepath):\n",
        "    \"\"\"\n",
        "    推論関数\n",
        "    \"\"\"\n",
        "    model.load_weights(filepath)\n",
        "    te_text = tokenize_texts(texts=test_df[Config.text_col].tolist(), tokenizer=tokenizer, max_length=Config.max_length)\n",
        "    te_dataset = get_dataset(X=te_text, y=None, dataset=\"test\")\n",
        "    preds = model.predict(te_dataset)\n",
        "    return preds.reshape(-1)\n",
        "\n",
        "\n",
        "def train_cv(train, cv, metrics, name, dir):\n",
        "    \"\"\"\n",
        "    cross validationの実行関数 (train)\n",
        "    \"\"\"\n",
        "    # oofの予測確率(probability)と最適化された閾値を保存\n",
        "    oof = np.zeros(len(train))\n",
        "    threshold_list = []\n",
        "\n",
        "    # fold training\n",
        "    for i_fold, (tr_idx, val_idx) in enumerate(cv):\n",
        "        K.clear_session()\n",
        "        print(f\"\\n===== FOLD {i_fold+1} training =====\")\n",
        "        filepath = os.path.join(dir, f\"{name}_fold{i_fold+1}.h5\")\n",
        "\n",
        "        # model, tokenizer, dataの準備\n",
        "        model, tokenizer = get_model_and_tokenizer()\n",
        "        tr_df, val_df = train.iloc[tr_idx].reset_index(), train.iloc[val_idx].reset_index()\n",
        "        \n",
        "\n",
        "        if not os.path.isfile(filepath):  # 学習済みモデルがあればtrainingしない\n",
        "            history = train_fn(tr_df, val_df, model, tokenizer, filepath)\n",
        "            pd.DataFrame(history.history).to_csv(\n",
        "                os.path.join(LOG, f\"history{time_jp}_{i_fold+1}.csv\"), \n",
        "                index=False)\n",
        "\n",
        "        # oofの予測確率を計算し、\n",
        "        # 最適化された閾値とスコアをpd.DataFrameとして取得\n",
        "        preds = inference_fn(val_df, model, tokenizer, filepath)\n",
        "        thre_df = get_result(val_df[Config.target_col], preds)\n",
        "\n",
        "        # 最適化された閾値\n",
        "        opted_thre = thre_df[thre_df.index==\"opted\"].threshold.values[0]\n",
        "\n",
        "        logger.info(f\"===== fold {i_fold+1} result =====\")\n",
        "        logger.info(f\">>> {thre_df.to_dict()}\")\n",
        "\n",
        "        # oofの予測確率値と最適化された閾値を保存\n",
        "        oof[val_idx] = preds\n",
        "        threshold_list.append(opted_thre)\n",
        "    \n",
        "    # oof全体の最適な閾値と、threshold_listから得た閾値でスコアを計算\n",
        "    thre_df = get_result(train[Config.target_col], oof, threshold_list)\n",
        "    logger.info(f\"===== total result =====\")\n",
        "    logger.info(f\">>> threshold:{thre_df.to_dict()['threshold']}\")\n",
        "    logger.info(f\">>> score:{thre_df.to_dict()['score']}\")\n",
        "    return oof, threshold_list\n",
        "\n",
        "\n",
        "def predict_cv(test, name, dir):\n",
        "    \"\"\"\n",
        "    cross validationの実行関数 (test)\n",
        "    \"\"\"\n",
        "    preds_fold = []\n",
        "    for i_fold in range(Config.n_fold):\n",
        "        filepath = os.path.join(dir, f\"{name}_fold{i_fold+1}.h5\")\n",
        "        model, tokenizer = get_model_and_tokenizer()\n",
        "\n",
        "        preds = inference_fn(test, model, tokenizer, filepath)\n",
        "        preds_fold.append(preds)\n",
        "\n",
        "        logger.info(f\"===== fold{i_fold+1} inference =====\")\n",
        "    \n",
        "    preds = np.mean(preds_fold, axis=0)\n",
        "    return preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MLt675RpWqS"
      },
      "source": [
        "###\n",
        "\n",
        "def submit_with_thresholds(type_list, threshold_list):\n",
        "    name = f\"{Config.model.replace('/','-')}_{time_jp}\"\n",
        "    prob_df = pd.read_csv(os.path.join(PROB, f\"prob_{name}.csv\"))\n",
        "    submit_df = pd.read_csv(os.path.join(INPUT, \"sample_submit.csv\"), \n",
        "                            header=None, \n",
        "                            names=[\"id\", \"judgement\"])\n",
        "    if Config.debug:\n",
        "        submit_df = submit_df.iloc[get_data(Config.test_file).index.values]\n",
        "        \n",
        "    # submit file\n",
        "    for key, threshold in zip(type_list, threshold_list):\n",
        "        predictions = (prob_df[f\"{name}_seed{Config.seeds[0]}\"].values >= threshold) * 1\n",
        "        filepath = f\"{name}_{key}.csv\"\n",
        "        submit_df[\"judgement\"] = predictions\n",
        "        submit_df.to_csv(os.path.join(SUBMIT, filepath), \n",
        "                         index=False, \n",
        "                         header=False)\n",
        "        logger.info(f\"saved file : {name}_{key}.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH6R-wJf-CDy"
      },
      "source": [
        "実行　main()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUI_ZPr6S2p3"
      },
      "source": [
        "def main():\n",
        "    model_name = f\"{Config.model.replace('/','-')}_{time_jp}\"\n",
        "    logger.info(f\"{model_name} TRAINING\")\n",
        "    # load data\n",
        "    train_df = get_data(Config.train_file)\n",
        "    test_df = get_data(Config.test_file)\n",
        "\n",
        "    # training\n",
        "    oof_df = pd.DataFrame()\n",
        "    threshold_list = []\n",
        "    for seed in Config.seeds:\n",
        "        name = f\"{model_name}_seed{seed}\"\n",
        "        logger.info(f\"***** SEED{seed} *****\")\n",
        "        oof, seed_thre_list = train_cv(\n",
        "            train_df, \n",
        "            cv=skf(train_df, n_splits=Config.n_fold, random_state=seed),\n",
        "            metrics=metrics, \n",
        "            name=name, \n",
        "            dir=MODEL)\n",
        "        oof_df[name] = oof\n",
        "        threshold_list += seed_thre_list\n",
        "\n",
        "    # save oof\n",
        "    oof_df.to_csv(os.path.join(PROB, f\"oof_{model_name}.csv\"), index=False)\n",
        "    logger.info(f\"saved file : oof_{model_name}.csv\")\n",
        "\n",
        "    # recode seeds total score\n",
        "    y_true = train_df[Config.target_col].values\n",
        "    y_pred = oof_df.mean(axis=1).values\n",
        "    thre_df = get_result(y_true, y_pred, threshold_list)\n",
        "    \n",
        "    logger.info(f\"***** seeds total result *****\")\n",
        "    logger.info(f\">>> threshold:{thre_df.to_dict()['threshold']}\")\n",
        "    logger.info(f\">>> score:{thre_df.to_dict()['score']}\")\n",
        "\n",
        "    # conf matrix with best threshold\n",
        "    opted_thre = thre_df[thre_df.index==\"opted\"].threshold.values[0]\n",
        "    fig = visualize_confusion_matrix(y_true, y_pred>=opted_thre)\n",
        "    fig.savefig(os.path.join(LOG, f\"cm_{time_jp}.png\"), dpi=300)\n",
        "\n",
        "    # save prob file\n",
        "    preds_df = pd.DataFrame()\n",
        "    for seed in Config.seeds:\n",
        "        name = f\"{model_name}_seed{seed}\"\n",
        "        preds = predict_cv(test_df, name, dir=MODEL)\n",
        "        preds_df[name] = preds\n",
        "\n",
        "    preds_df.to_csv(os.path.join(PROB, f\"prob_{model_name}.csv\"), index=False)  # test予測値(prob)を保存\n",
        "    logger.info(f\"saved file : prob_{name}.csv\")\n",
        "\n",
        "    # submit\n",
        "    if Config.submit:\n",
        "        submit_with_thresholds(thre_df.index, thre_df.threshold)\n",
        "\n",
        "    return thre_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdFJ4htCK-lx"
      },
      "source": [
        "## execute"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51KXC98yt7CB"
      },
      "source": [
        "thre_df = main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EF-WygLLyBTf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c80dde58-c67d-4358-a5f7-0cfd6edbff78"
      },
      "source": [
        "code_text = \"\"\n",
        "with open(os.path.join(DRIVE, \"medical_bert_tf.ipynb\"), mode=\"r\") as f:\n",
        "    code = f.read()\n",
        "\n",
        "for i in [2,3]:\n",
        "    code_text += \"\".join(json.loads(code)[\"cells\"][i][\"source\"])+\"\\n\\n\"\n",
        "\n",
        "logger.info(code_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:[2021-10-04 22:22:41] - class Config:\n",
            "    model = \"dmis-lab/biobert-base-cased-v1.2\" #@param\n",
            "    from_pt = True #@param {\"type\":\"boolean\"}\n",
            "    encode_type = \"cls_cat\" #@param {\"type\",\"string\"} [\"cls\",\"cls_cat\",\"pooler\",\"logits\", \"last_hidden_state_cnn\", \"last_hidden_state_lstm\"]\n",
            "\n",
            "    max_length = 512 #@param {\"type\":\"integer\"}\n",
            "    lr = 0.00002\n",
            "    weight_decay = 1e-5\n",
            "    opt = \"minimize\"  #@param {\"type\":\"string\"} [\"minimize_scalar\",\"minimize\"]\n",
            "    n_fold = 5 #@param\n",
            "    epochs = 15 #@param {\"type\":\"slider\"}\n",
            "    patience =  4#@param\n",
            "    check_monitor = \"val_fbeta_score\" #@param {\"type\":\"string\"} [\"val_loss\",\"val_fbeta_score\",\"val_auc\"]\n",
            "    check_mode = \"max\" #@param {\"type\":\"string\"} [\"auto\", \"max\"]\n",
            "    \n",
            "    train_batch_size = 64 #@param {\"type\":\"raw\"} [4,8,16,32,64]\n",
            "    valid_batch_size = 64 #@param {\"type\":\"raw\"} [4,8,16,32,64]\n",
            "    test_batch_size = 64 #@param {\"type\":\"raw\"} [4,8,16,32,64]\n",
            "    steps_per_epochs = None #(27145 * (n_fold - 1) / n_fold) // train_batch_size\n",
            "    train_file = \"ps_train.csv\" #@param\n",
            "    test_file = \"ps_test.csv\" #@param\n",
            "    target_col = \"judgement\"\n",
            "    text_col = \"summary\"  #@param {\"type\":\"string\"} [\"text\",\"summary\"]\n",
            "    seeds = [21]\n",
            "\n",
            "    loss_fn = \"bce\" #@param {\"type\":\"string\"} [\"bce\", \"weighted_bce\", \"focal\"]\n",
            "    loss_weight = [1,50] #@param\n",
            "    class_weight = \"balanced\" #@param {\"type\":\"raw\"} \n",
            "    sample_weight = None #@param    \n",
            "    label_smoothing = 0 #@param\n",
            "    \n",
            "    submit = True #@param {\"type\":\"boolean\"}\n",
            "    debug = False  #@param {\"type\":\"boolean\"}\n",
            "    temp_thre = 0.1 #@param\n",
            "\n",
            "if Config.debug:\n",
            "    Config.epochs = 2\n",
            "    Config.n_fold = 2\n",
            "\n",
            "time_jp = (datetime.datetime.now() + \n",
            "           datetime.timedelta(hours=9)).strftime('%Y%m%d_%H%M')\n",
            "# time_jp = '20210926_1749' #@param\n",
            "time_jp\n",
            "\n",
            "def build_auto_model():\n",
            "    \n",
            "    # encoder\n",
            "    if Config.encode_type == \"logits\":\n",
            "        encoder = (\n",
            "            transformers\n",
            "            .TFAutoModelForSequenceClassification\n",
            "            .from_pretrained(Config.model, num_labels=1, from_pt=Config.from_pt)\n",
            "        )\n",
            "    elif Config.encode_type == \"cls_cat\":\n",
            "        config = transformers.AutoConfig.from_pretrained(Config.model,\n",
            "                                                         output_hidden_states=True)\n",
            "        encoder = (\n",
            "            transformers\n",
            "            .TFAutoModel\n",
            "            .from_pretrained(Config.model, config=config, from_pt=Config.from_pt)\n",
            "        )\n",
            "\n",
            "    else:\n",
            "        encoder = (\n",
            "            transformers\n",
            "            .TFAutoModel\n",
            "            .from_pretrained(Config.model, from_pt=Config.from_pt)\n",
            "        )\n",
            "    \n",
            "    input_word_ids = tf.keras.layers.Input(shape=(Config.max_length, ), dtype=tf.int32, name='input_ids')\n",
            "    attention_mask = tf.keras.layers.Input(shape=(Config.max_length, ), dtype=tf.int32, name='attention_mask')\n",
            "    x = encoder(input_word_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
            "\n",
            "    # output token type\n",
            "    if Config.encode_type == \"cls\":\n",
            "        x = x[0][:, 0, :]  # cls token\n",
            "        output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
            "\n",
            "    elif Config.encode_type == \"cls_cat\":\n",
            "        x = tf.concat([x[\"hidden_states\"][-i][:,0,:] for i in range(1,5)], axis=-1)\n",
            "        x = tf.keras.layers.Dropout(0.2)(x)\n",
            "        output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
            "\n",
            "    elif Config.encode_type == \"pooler\":\n",
            "        x = x[\"pooler_output\"]\n",
            "        output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
            "\n",
            "    elif Config.encode_type == \"logits\":\n",
            "        x = x.logits\n",
            "        output = tf.keras.layers.Activation(\"sigmoid\")(x)\n",
            "\n",
            "    elif Config.encode_type == \"last_hidden_state_cnn\":\n",
            "        x = x.last_hidden_state\n",
            "        x = tf.keras.layers.Conv1D(\n",
            "            256, kernel_size=2, padding=\"same\", activation=\"relu\")(x)\n",
            "        x = tf.keras.layers.Conv1D(\n",
            "            1, kernel_size=2, padding=\"same\")(x)\n",
            "        x = tf.keras.layers.GlobalMaxpooling1D()(x)\n",
            "        output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
            "        \n",
            "    elif Config.encode_type == \"last_hidden_state_lstm\":\n",
            "        x = x.last_hidden_state\n",
            "        x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(728))(x)\n",
            "        output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
            "        \n",
            "\n",
            "    model = tf.keras.models.Model(inputs=[input_word_ids, attention_mask],\n",
            "                                  outputs=[output])\n",
            "\n",
            "    optimizer = tfa.optimizers.AdamW(lr=Config.lr, weight_decay=Config.weight_decay)\n",
            "\n",
            "    loss = {\"bce\": tf.keras.losses.BinaryCrossentropy(),\n",
            "            \"weighted_bce\": weighted_binary_crossentropy(Config.loss_weight, Config.label_smoothing),\n",
            "            \"focal\": tfa.losses.SigmoidFocalCrossEntropy(alpha=0.98, gamma=2.0),\n",
            "            \"mse\": tf.keras.losses.MeanSquaredError()}\n",
            "\n",
            "    metrics = [tfa.metrics.FBetaScore(num_classes=1,\n",
            "                                      beta=7.0,\n",
            "                                      threshold=Config.temp_thre),\n",
            "               tf.keras.metrics.AUC(num_thresholds=200, curve='PR',\n",
            "                                    multi_label=False, label_weights=None)]\n",
            "\n",
            "    model.compile(optimizer=optimizer,\n",
            "                  loss=loss[Config.loss_fn], \n",
            "                  metrics=metrics)\n",
            "    # model.summary()\n",
            "    return model\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_wJ4T6toOsy"
      },
      "source": [
        "## submit with thresholds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAA1jKWujyDG",
        "outputId": "0e78a3b6-90de-4513-d509-e9deae07bf98"
      },
      "source": [
        "submit_with_thresholds(thre_df.index, thre_df.threshold)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:[2021-09-20 22:13:55] - saved file : cambridgeltl-BioRedditBERT-uncased_20210920_1742_opted.csv\n",
            "INFO:__main__:[2021-09-20 22:13:55] - saved file : cambridgeltl-BioRedditBERT-uncased_20210920_1742_temporary.csv\n",
            "INFO:__main__:[2021-09-20 22:13:55] - saved file : cambridgeltl-BioRedditBERT-uncased_20210920_1742_min.csv\n",
            "INFO:__main__:[2021-09-20 22:13:55] - saved file : cambridgeltl-BioRedditBERT-uncased_20210920_1742_max.csv\n",
            "INFO:__main__:[2021-09-20 22:13:55] - saved file : cambridgeltl-BioRedditBERT-uncased_20210920_1742_mean.csv\n"
          ]
        }
      ]
    }
  ]
}